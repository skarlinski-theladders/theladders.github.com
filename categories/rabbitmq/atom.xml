<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: RabbitMQ | TheLadders Engineering Stories]]></title>
  <link href="http://dev.theladders.com/categories/rabbitmq/atom.xml" rel="self"/>
  <link href="http://dev.theladders.com/"/>
  <updated>2015-07-31T17:14:10-04:00</updated>
  <id>http://dev.theladders.com/</id>
  <author>
    <name><![CDATA[TheLadders Engineering]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Monitoring RabbitMQ at TheLadders]]></title>
    <link href="http://dev.theladders.com/2014/07/monitoring-rabbitmq-at-theladders/"/>
    <updated>2014-07-15T10:24:00-04:00</updated>
    <id>http://dev.theladders.com/2014/07/monitoring-rabbitmq-at-theladders</id>
    <content type="html"><![CDATA[<p><blockquote><p>Alice:        “How long is forever?”</p></p><p><p>White Rabbit:     “Sometimes, just one second.&ldquo;</p><footer><strong>&mdash;Lewis Carroll</strong> <cite>Alice in Wonderland</cite></footer></blockquote></p>

<h2><em>Motivation</em></h2>

<p>TheLadders makes heavy use of <a href="http://www.rabbitmq.com">RabbitMQ</a> in its operations. RabbitMQ is a messaging broker that we use to connect our applications. Sending and receiving are separate, so the messaging is asynchronous. The asynchronous messaging decouples one application from another. We use RabbitMQ to publish events to multiple subscribers and to queue up work.</p>

<ul>
<li>In some cases, events in our systems are ad hoc, driven by the various activities of our users. Examples of such events include the addition of a job seeker or a job to the system. These events are realized as RabbitMQ messages posted to exchanges. Systems interested in these events read them from queues bound to the exchanges.</li>
<li>In other cases, a scheduled process posts RabbitMQ messages to an exchange so that a specific system (say, a <a href="https://storm.incubator.apache.org">Storm</a> topology) can process them. An example of this is a system that calculates matches between jobs and job seekers and puts those matches on a queue. A Storm topology reads those matches from the queue and transforms them into emails to be sent by an external email service.</li>
</ul>


<p>In the first scenario above, we are mostly interested in making sure that the queue of ad hoc events has not grown too large, which would indicate a problem with the system that is consuming the messages.</p>

<p>In the second scenario, we want to be sure that messages are being published to the queue at a suitable rate, and that the topology that consumes the messages is processing them quickly enough to get the emails out by a particular time.</p>

<p>We did not find a suitable ready-to-use solution for this need. As you will see below, we eventually arrived at a solution that combined a <a href="http://clojure.org/">Clojure</a> library for accessing RabbitMQ queue metrics with a <a href="http://riemann.io">Riemann</a> server to process events produced by the library.</p>

<h2><em>First Attempts</em></h2>

<p>Our first attempts at a monitoring solution revolved around an assortment of shell scripts that made HTTP requests to the RabbitMQ Management API to discover the lengths of important queues at particular times. If certain thresholds were crossed, alerts were sent to on-call personnel. This approach had a number of drawbacks:</p>

<ul>
<li>Brittleness: The times of the Management API queries were controlled by when the script was run by cron. The queue length thresholds were often dependent on the sizes of the datasets involved and the speed of the systems which published messages or read them from the queues.</li>
<li>Measuring Queue Lengths at a Single Point in Time: Checking the queue length at a particular point in time could not provide the information necessary to answer questions such as &ldquo;When will this queue be drained?&rdquo; and &ldquo;Are messages being placed on the queue at a rate sufficient to meet a particular processing schedule?&rdquo;</li>
</ul>


<h2><em>How We Solved the Problem</em></h2>

<p>We created a <a href="http://clojure.org/">Clojure</a> library called <a href="https://github.com/TheLadders/monitor-rabbitmq">monitor-rabbitmq</a>, available on GitHub.
It gathers statistics on RabbitMQ queues and nodes, and packages them as <a href="http://riemann.io">Riemann</a> events. <em>monitor-rabbitmq</em> takes care of acquiring RabbitMQ statistics from the RabbitMQ cluster, using the RabbitMQ’s HTTP based <a href="http://hg.rabbitmq.com/rabbitmq-management/raw-file/rabbitmq_v3_3_4/priv/www/api/index.html">Management API</a>. Riemann takes care of aggregating the events and performing the calculations necessary to determine whether an alert should be triggered.</p>

<hr />

<p><em>A Bit More Detail</em></p>

<p><strong>What’s the data flow?</strong></p>

<p><img class="center" src="/images/monitor-rabbitmq2.png" title="&lsquo;Data Flow graphic&rsquo;" ></p>

<p><strong>What queue data does</strong> <em>monitor-rabbitmq</em> <strong>gather?</strong> [all rates are messages per second]</p>

<ul>
<li>Queue Length: number of messages on the queue</li>
<li>Ack rate: messages acknowledged by the client</li>
<li>Deliver rate: notifications to the client that there is a message</li>
<li>Deliver-get rate: combination of deliver and get rates</li>
<li>Deliver-no-ack rate: notifications to the client that there is a message which does not require acknowledgement</li>
<li>Get rate: messages synchronously requested by client</li>
<li>Get-no-ack rate: messages sent to client with no acknowledgement required</li>
<li>Publish rate: messages published to the queue</li>
<li>Redeliver rate: messages redelivered to the client after being delivered and not acked</li>
</ul>


<p><strong>What node data does</strong> <em>monitor-rabbitmq</em> <strong>gather?</strong></p>

<ul>
<li>fd_used: file descriptors used</li>
<li>fd_total: file descriptors total</li>
<li>sockets_used</li>
<li>sockets_total</li>
<li>mem_used</li>
<li>mem_limit</li>
<li>mem_alarm: memory high water mark alarm</li>
<li>disk_free</li>
<li>disk_free_limit</li>
<li>disk_free_alarm: disk free low water mark alarm</li>
<li>proc_used</li>
<li>proc_total</li>
</ul>


<p><strong>What does each</strong> Riemann <strong>event look like?</strong>
Here is the Clojure representation (a map):
```clj
{:time 1390593087006,</p>

<pre><code>:host "our-rabbitmq.super.awesome.queue",
:service "publish.rate",
:metric 0.0,
:state "ok",
:tags ["rabbitmq"]}
</code></pre>

<p><code>
This event, created by the Clojure library, includes a</code>:host<code>member which is formed by taking a</code>rmq-display-name argument``` (“our-rabbitmq”) and composing it with the queue (or node) name: “super.awesome.queue”</p>

<h2><em>An Example</em></h2>

<p>Let&rsquo;s say that we have a queue which is read by a Storm topology. That queue contains messages which hold matches between jobs and job seekers. The topology must process the messages so that emails notifying job seekers of these job opportunities are composed and sent to an external email service. We want to be alerted if the messages are being consumed from the queue at a rate such that the queue will not be cleared by a certain deadline, say 9:00 AM.</p>

<p>We create a simple Clojure application that calls the <code>send-nodes-stats-to-riemann</code> function of our library and then exits. We call this application once a minute. Each call results in a full set of queue statistics being sent to our Riemann server.</p>

<p>Meanwhile, on the Riemann side of things, Riemann has been configured to watch the Ack rate of our queue. Riemann accumulates events over a time interval, smoothing out minor variations in the Ack rate, and calculates a projected time for the queue to be emptied. If the Ack rate dips below a certain threshold, it triggers an alert using, in our environment, the <a href="https://www.icinga.org">Icinga</a> monitoring system.</p>

<h2><em>Wrapping Up</em></h2>

<p>With <em>monitor-rabbitmq</em>, we supply a regular flow of events to our Riemann server containing data about our RabbitMQ queues and nodes. By choosing a good base set of statistics to request from the RabbitMQ Management API, a change to our monitoring and alerting requirements usually results in only a change to our Riemann configuration.</p>

<p>Find this useful? Join the discussion at <a href="https://news.ycombinator.com/item?id=8036593">Hacker News</a>.</p>
]]></content>
  </entry>
  
</feed>
